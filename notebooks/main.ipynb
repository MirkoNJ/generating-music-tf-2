{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hollow-dispatch",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-highway",
   "metadata": {},
   "source": [
    "#### Standard library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-platform",
   "metadata": {},
   "source": [
    "#### Third party imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-birth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_probability as tfp\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-pressure",
   "metadata": {},
   "source": [
    "#### Local imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.batch as batch\n",
    "import modules.midi_related as midi\n",
    "import modules.preprocessing as prep\n",
    "import modules.subclasses as sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-language",
   "metadata": {},
   "source": [
    "#### Extensions and autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-development",
   "metadata": {},
   "source": [
    "#### Dis-/enable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_gpu = False\n",
    "debugging = False\n",
    "\n",
    "if disable_gpu:\n",
    "    # Hide GPU from visible devices\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    if debugging:\n",
    "        # To find out which devices your operations and tensors are assigned to\n",
    "        tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "        # Create some tensors and perform an operation\n",
    "        a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "        b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "        c = tf.matmul(a, b)\n",
    "\n",
    "        print(c)\n",
    "        tf.debugging.set_log_device_placement(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-street",
   "metadata": {},
   "source": [
    "#### Setting relative directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "Working_Directory = os.getcwd()\n",
    "Project_Directory = os.path.abspath(os.path.join(Working_Directory,'..'))\n",
    "Music_In_Directory = Project_Directory + \"/data/chopin_midi/\"\n",
    "Output_Directory = Project_Directory + \"/outputs/\"\n",
    "Model_Directory = Output_Directory + \"models/\"\n",
    "Checkpoint_Directory = Model_Directory + \"ckpt/\"\n",
    "Numpy_Directory = Model_Directory + \"arrays/\"\n",
    "Music_Out_Directory = Output_Directory + \"midi/\"\n",
    "Music_Out_Training_Directory =  Music_Out_Directory + \"train/\"\n",
    "Music_Out_Genereating_Directory = Music_Out_Directory + \"generated/\"\n",
    "#Log_Directory = Output_Directory + \"logs/fit/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-pastor",
   "metadata": {},
   "source": [
    "#### Tensorboard callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently not in use\n",
    "\n",
    "#current_time_str = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#log_dir = Log_Directory + current_time_str\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-lesson",
   "metadata": {},
   "source": [
    "### Data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-cutting",
   "metadata": {},
   "source": [
    "#### Load pieces (i.e. import midi files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First checkt that importing single midi (i.e. Chopin Op 28 No.4) works\n",
    "chop2804 = midi.midiToNoteStateMatrix(Music_In_Directory + \"chop2804.mid\", verbose=False, verbose_ts=False) \n",
    "# verbose = true, should help understand how the import works\n",
    "# verbose_ts = true, prints resolution of midi file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all Chopin data\n",
    "min_time_steps = 128 # only files with at least this many 48th note steps are saved\n",
    "\n",
    "# Gather the training pieces from the specified directory\n",
    "training_pieces={}\n",
    "training_pieces = {**training_pieces, **midi.loadPieces(Music_In_Directory, min_time_steps, False)}\n",
    "print('Number of total pieces = ', len(training_pieces))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if writing back as midi file works\n",
    "check_backtransform_midi = False #True\n",
    "if check_backtransform_midi:\n",
    "    out = tf.convert_to_tensor(training_pieces['chop2804'], dtype=tf.int32)\n",
    "    midi.noteStateMatrixToMidi(out, Music_Out_Directory + \"chop2804_backtransformed\", tickscale = 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-favorite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table of piece names for midi files loaded\n",
    "df_chopin_opus = pd.read_csv(Project_Directory + '/data/chopin_title_opus.csv')\n",
    "op_numbers = [int(str(k)[4:6]) for k in training_pieces.keys()]\n",
    "op_numbers_unique = set(op_numbers)\n",
    "df_chopin_opus_midi = df_chopin_opus.loc[df_chopin_opus['Op'].isin(op_numbers_unique)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define genres used for training\n",
    "genres = ['Ballade', 'Etudes', 'promptu', 'Mazurkas', \n",
    "          'Nocturnes', 'Preludes',  'Sonata', 'Waltzes']\n",
    "df_pieces_summary = pd.concat([midi.get_piece_summary_df(i, df_chopin_opus_midi, op_numbers) for i in genres])\n",
    "df_genre_summary = df_pieces_summary[['MIDI files', 'genre', 'Op']] \\\n",
    "                    .groupby(['genre']) \\\n",
    "                    .agg({'Op': list, 'MIDI files': sum})\n",
    "df_genre_summary.rename(index={'Ballade':'Ballades','promptu':'Impromptus', 'Sonata' : 'Sonatas'}, inplace=True)\n",
    "df_genre_summary = df_genre_summary.sort_index()\n",
    "#genre_summary.style.set_properties(subset=['Op'], **{'width-min': '300px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pieces for each genre\n",
    "ballades = midi.get_subset_training_pieces_for_genre('Ballades', training_pieces, df_genre_summary)\n",
    "etudes = midi.get_subset_training_pieces_for_genre('Etudes', training_pieces, df_genre_summary)\n",
    "impromptus = midi.get_subset_training_pieces_for_genre('Impromptus', training_pieces, df_genre_summary)\n",
    "mazurkas = midi.get_subset_training_pieces_for_genre('Mazurkas', training_pieces, df_genre_summary)\n",
    "nocturnes = midi.get_subset_training_pieces_for_genre('Nocturnes', training_pieces, df_genre_summary)\n",
    "preludes = midi.get_subset_training_pieces_for_genre('Preludes', training_pieces, df_genre_summary)\n",
    "sonatas = midi.get_subset_training_pieces_for_genre('Sonatas', training_pieces, df_genre_summary)\n",
    "waltzes = midi.get_subset_training_pieces_for_genre('Waltzes', training_pieces, df_genre_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update training_pieces to only use pieces from defined genres \n",
    "op_train = [item for sublist in df_genre_summary['Op'] for item in sublist]\n",
    "training_keys = [k for k in training_pieces.keys() if int(str(k)[4:6]) in op_train]\n",
    "training_pieces = {k:v for k, v in training_pieces.items() if k in training_keys}\n",
    "#training_pieces = mazurkas\n",
    "print('Number of total pieces left = ', len(training_pieces))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-settlement",
   "metadata": {},
   "source": [
    "#### Train/Validation pieces split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either select one validation and one training piece or set aside a random set of pieces for validation purposes\n",
    "single_piece = False\n",
    "if single_piece:\n",
    "    validation_pieces = {'chop2803' : training_pieces['chop2803']}\n",
    "    training_pieces   = {'chop2804' : training_pieces['chop2804']}\n",
    "else:\n",
    "    num_validation_pieces = len(training_pieces) // 10\n",
    "\n",
    "    validation_pieces={}\n",
    "    for v in range(num_validation_pieces):\n",
    "        index = random.choice(list(training_pieces.keys()))\n",
    "        validation_pieces[index] = training_pieces.pop(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of training   pieces = ', len(training_pieces))    \n",
    "print('Number of validation pieces = ', len(validation_pieces))     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-sydney",
   "metadata": {},
   "source": [
    "#### Check that features (X) and lables (y) generation work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample Note State Matrix for dimension measurement and numerical checking purposes\n",
    "sample_size = 16\n",
    "sample_num_timesteps = 144\n",
    "Midi_low = 21\n",
    "Midi_high = 108\n",
    "\n",
    "y = batch.getPieceBatch(training_pieces, sample_size, sample_num_timesteps) \n",
    "X = prep.inputKernel(y, Midi_low, Midi_high)\n",
    "\n",
    "\n",
    "print('Dimensions y: (sample_size, num_notes, num_timesteps, play_articulate_velocity) = ', y.shape)\n",
    "print('Dimensions X: (sample_size, num_notes, num_timesteps, feature_dim             ) = ', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature vector for the 1st batch the 53rd note and the 51st 48th-note timestep (cast to int)\n",
    "prep.noteRNNInputSummary(X[0,30,50,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-bunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to all notes played at timestep 51\n",
    "[(i,j) for i,j in enumerate(list(tf.cast(y[0,:,50,:], dtype=tf.int32).numpy())) if j[0]== 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-liquid",
   "metadata": {},
   "source": [
    "#### Test generating midi files from y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtransform the 128 48th notes from the first random sampled training_piece to MIDI\n",
    "test_midi = tf.cast(tf.transpose(y, perm=[0,2,1,3])[0,:,:,:], dtype=tf.int32).numpy()\n",
    "midi.noteStateMatrixToMidi(test_midi, Music_Out_Directory + \"random_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-recycling",
   "metadata": {},
   "source": [
    "#### Check number of notes played, articulated and velocity per timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tf.transpose(y, perm=[0,2,1,3])[0,:,:,:].numpy()\n",
    "notes_per_timestep = [(i,tf.reduce_sum(test[i,:,0]).numpy()) for i in range(test.shape[0])]\n",
    "articulate_per_timestep = [(i,tf.reduce_sum(test[i,:,1]).numpy()) for i in range(test.shape[0])]\n",
    "mean_tempo_per_timestep = [(i,tf.reduce_mean(test[i,np.array(test[i,:,0], dtype=bool),2]).numpy()) for i in range(test.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-voluntary",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-recycling",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_notes = 88      # X.shape[1] = Midi_high + 1 - Midi_low \n",
    "num_timesteps = -1  # keep the num_timesteps variable, in training set to X.shape[2]\n",
    "input_size = 108     # X.shape[3]\n",
    "drop_out_rate = 0.5\n",
    "\n",
    "num_t_units = [128, 128] # [256, 128]\n",
    "num_n_units = [64,  64] # [128,  64]\n",
    "dense_units = 3    #(play,articulate,velocity)\n",
    "\n",
    "TIME_BATCH_SHAPE = (num_timesteps, input_size)\n",
    "NOTE_BATCH_SHAPE = (num_notes, num_t_units[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-colon",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-longer",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(num_notes, None, input_size), name=\"inputs\")\n",
    "inputs_shape = sub.GetShape()(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-angola",
   "metadata": {},
   "source": [
    "#### Timewise LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping\n",
    "x = sub.BatchReshape(TIME_BATCH_SHAPE, False)(inputs)\n",
    "\n",
    "# Timewise LSTMs\n",
    "x = tf.keras.layers.LSTM(num_t_units[0], return_sequences=True, dropout= drop_out_rate)(x)\n",
    "x = tf.keras.layers.LSTM(num_t_units[1], return_sequences=True, dropout= drop_out_rate)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-casino",
   "metadata": {},
   "source": [
    "#### Notewise LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-anderson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping\n",
    "x = sub.BatchReshape([num_notes, num_timesteps , num_t_units[1]], True)(x)\n",
    "x = tf.keras.layers.Permute((2,1,3))(x)\n",
    "x = sub.BatchReshape(NOTE_BATCH_SHAPE, False)(x)\n",
    "\n",
    "# Notewise LSTMs\n",
    "x = tf.keras.layers.LSTM(num_n_units[0], return_sequences=True, dropout= drop_out_rate)(x)\n",
    "x = tf.keras.layers.LSTM(num_n_units[1], return_sequences=True, dropout= drop_out_rate)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-multiple",
   "metadata": {},
   "source": [
    "#### Simulate conditional probabilty using dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sub.SliceNotesTensor()(x)\n",
    "\n",
    "x_tmp = tf.keras.layers.Dense(units=dense_units, activation=None)(x[0])\n",
    "note = sub.SampleNote()(x_tmp)\n",
    "\n",
    "x_list = [x_tmp]\n",
    "note_list = [sub.SliceNoteVelocityTensor()(note)]\n",
    "\n",
    "for n in range(1,len(x)):\n",
    "    x_tmp = tf.keras.layers.Concatenate(axis=-1)([x[n], note])\n",
    "    x_tmp = tf.keras.layers.Dense(units=dense_units, activation=None)(x_tmp)\n",
    "    note = sub.SampleNote()(x_tmp)\n",
    "    x_list.append(x_tmp)\n",
    "    note_list.append(sub.SliceNoteVelocityTensor()(note))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-begin",
   "metadata": {},
   "source": [
    "#### Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output 1    \n",
    "x_list = sub.ExpandDims()(x_list, axis=1)\n",
    "x = tf.keras.layers.Concatenate(axis=1)(x_list)\n",
    "x = sub.BatchReshape([num_timesteps, num_notes , dense_units], True)(x, inputs_shape)\n",
    "x_1, x_2 = sub.SliceNotesVelocityTensor()(x)\n",
    "output_1 = tf.keras.layers.Permute((2,1,3), name='play_articulate_prob')(x_1)\n",
    "\n",
    "#Output 2\n",
    "x_2 = sub.BackTransformVelocity()(x_2)\n",
    "output_2 = tf.keras.layers.Permute((2,1,3), name='velocity')(x_2)\n",
    "\n",
    "# Output 3\n",
    "note_list = sub.ExpandDims()(note_list, axis=1)\n",
    "note = tf.keras.layers.Concatenate(axis=1)(note_list)\n",
    "note = sub.BatchReshape([num_timesteps, num_notes , 2], True)(note, inputs_shape)\n",
    "output_3 = tf.keras.layers.Permute((2,1,3), name='play_articulate_sampled')(note)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-makeup",
   "metadata": {},
   "source": [
    "#### Full LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = tf.keras.Model(inputs, [output_1, output_2, output_3], name=\"full\")\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-denver",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = False\n",
    "if plot:\n",
    "    plot_res = tf.keras.utils.plot_model(lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-exhaust",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-thought",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = 144\n",
    "\n",
    "# loss\n",
    "losses = (sub.CustomSigmoidFocalCrossEntropy(from_logits = True, \n",
    "                                           gamma = 0, \n",
    "                                           alpha = 0),\n",
    "          sub.MeanSquaredErrorVelocity())\n",
    "\n",
    "# metric\n",
    "metrics = (sub.CustomBinaryAccuracy(threshold=0.5), \n",
    "           tfa.metrics.MultiLabelConfusionMatrix(num_classes=2),\n",
    "           sub.root_mean_squared_error_velocity_metric)\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.keras.optimizers.Adadelta(learning_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The tf.function \n",
    "@tf.function\n",
    "def train_on_batch(X, y):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        y_pred_p_a, y_pred_velocity, sampled_p_a = lstm(X, training=True)\n",
    "        \n",
    "        loss_p_a        = losses[0](y, y_pred_p_a)\n",
    "        loss_velocity   = losses[1](y, y_pred_velocity)\n",
    "        loss_total      = loss_p_a + tf.math.sqrt(loss_velocity) / 127 #scale the loss to same scale???\n",
    "        \n",
    "        metric_p_a      = metrics[0](y, y_pred_p_a)\n",
    "        confs           = metrics[1](tf.reshape(y[:,:,:,0:2],  [-1,2]), \n",
    "                                    tf.reshape(sampled_p_a, [-1,2]))\n",
    "        p_conf, a_conf  = confs[0], confs[1]\n",
    "        metric_velocity = metrics[2](y, y_pred_velocity)\n",
    "        \n",
    "    grads = tape.gradient(loss_total, lstm.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, lstm.trainable_weights))\n",
    "    \n",
    "    return (tf.reduce_mean(loss_p_a), \n",
    "            tf.reduce_mean(tf.math.sqrt(loss_velocity) / 127), \n",
    "            tf.reduce_mean(metric_p_a), \n",
    "            tf.reduce_mean(metric_velocity), \n",
    "            [p_conf, a_conf])\n",
    "\n",
    "@tf.function\n",
    "def validate_on_batch(X, y):\n",
    "    \n",
    "    y_pred_p_a, y_pred_velocity, sampled_p_a = lstm(X, training=False)\n",
    "    \n",
    "    loss_p_a       = losses[0](y, y_pred_p_a)\n",
    "    loss_velocity  = losses[1](y, y_pred_velocity)\n",
    "    loss_total     = loss_p_a + tf.math.sqrt(loss_velocity) / 127\n",
    "    \n",
    "    metric_p_a     = metrics[0](y, y_pred_p_a)\n",
    "    confs          = metrics[1](tf.reshape(y[:,:,:,0:2],  [-1,2]), \n",
    "                                tf.reshape(sampled_p_a, [-1,2]))\n",
    "    p_conf, a_conf = confs[0], confs[1]    \n",
    "    metric_velocity = metrics[2](y, y_pred_velocity)\n",
    "    \n",
    "    return (tf.reduce_mean(loss_p_a), \n",
    "            tf.reduce_mean(tf.math.sqrt(loss_velocity) / 127), \n",
    "            tf.reduce_mean(metric_p_a), \n",
    "            tf.reduce_mean(metric_velocity), \n",
    "            [p_conf, a_conf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_batches = 2048\n",
    "n_val_batches = 256\n",
    "\n",
    "epochs = 16\n",
    "batch_size = 2\n",
    "epoch_save_list = [1, 2, 4, 8 ,16, 32, 64, 128, 256, 512]\n",
    "\n",
    "# Values for loss, metric and confusion matrix\n",
    "train_loss_p_a_array   = np.full((epochs, n_train_batches), 10.0)\n",
    "train_loss_vel_array   = np.full((epochs, n_train_batches), 10.0)\n",
    "train_metric_p_a_array = np.full((epochs, n_train_batches), 10.0)\n",
    "train_metric_vel_array = np.full((epochs, n_train_batches), 10.0)\n",
    "val_loss_p_a_array     = np.full((epochs, n_val_batches), 10.0)\n",
    "val_loss_vel_array     = np.full((epochs, n_val_batches), 10.0)\n",
    "val_metric_p_a_array   = np.full((epochs, n_val_batches), 10.0)\n",
    "val_metric_vel_array   = np.full((epochs, n_val_batches), 10.0)\n",
    "train_p_conf_array     = np.full((epochs, 2, 2), 10.0)\n",
    "train_a_conf_array     = np.full((epochs, 2, 2), 10.0)\n",
    "val_p_conf_array       = np.full((epochs, 2, 2), 10.0)\n",
    "val_a_conf_array       = np.full((epochs, 2, 2), 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CUSTOM TRAINING LOOP\n",
    "\n",
    "# Timing\n",
    "start_time = time.time()\n",
    "time_old = start_time\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('\\rStart of Epoch [%d/%d]'% (epoch + 1, epochs))\n",
    "    print('\\n')\n",
    "\n",
    "    #ensure that every epoch the same training data is used\n",
    "    random.seed(1337)\n",
    "    for n in range(n_train_batches):\n",
    "        print('Training batch: %d/%d' % (n + 1, n_train_batches), end='\\r')\n",
    "        train_dataset = prep.createDataSet(training_pieces, batch_size, num_timesteps, batch_size)\n",
    "        for _, (X_train, y_train) in enumerate(train_dataset):\n",
    "            l_1, l_2, m_1, m_2, confusion_mat = train_on_batch(X_train, y_train)\n",
    "            train_loss_p_a_array[epoch, n]    = l_1\n",
    "            train_loss_vel_array[epoch, n]    = l_2\n",
    "            train_metric_p_a_array[epoch, n]  = m_1\n",
    "            train_metric_vel_array[epoch, n]  = m_2\n",
    "    print('')\n",
    "    \n",
    "    # storing the confusion matrix of validation set for predicting play/ articulate\n",
    "    train_p_conf_array[epoch, : , :] = confusion_mat[0]\n",
    "    train_a_conf_array[epoch, : , :] = confusion_mat[1]\n",
    "\n",
    "    # reset metrics\n",
    "    metrics[0].reset_states()\n",
    "    metrics[1].reset_states()\n",
    "\n",
    "    for n in range(n_val_batches):\n",
    "        print('Validation batch: %d/%d' % (n + 1, n_val_batches), end='\\r')\n",
    "        val_dataset   = prep.createDataSet(validation_pieces, batch_size, num_timesteps, batch_size)\n",
    "        for _, (X_val, y_val) in enumerate(val_dataset):\n",
    "            l_1, l_2, m_1, m_2, confusion_mat = validate_on_batch(X_val, y_val)\n",
    "            val_loss_p_a_array[epoch, n]      = l_1\n",
    "            val_loss_vel_array[epoch, n]      = l_2\n",
    "            val_metric_p_a_array[epoch, n]    = m_1 \n",
    "            val_metric_vel_array[epoch, n]    = m_2 \n",
    "\n",
    "    print('')\n",
    "    print('Seed:' + str(random.randrange(0,1000000,1)))\n",
    "\n",
    "    # storing the confusion matrix of validation set for predicting play/ articulate\n",
    "    val_p_conf_array[epoch, : , :] = confusion_mat[0]\n",
    "    val_a_conf_array[epoch, : , :] = confusion_mat[1]\n",
    "\n",
    "    # reset metrics\n",
    "    metrics[0].reset_states()\n",
    "    metrics[1].reset_states()\n",
    "    \n",
    "    print('Training   Loss p_a: '     + str(np.mean(train_loss_p_a_array[epoch,:])))\n",
    "    print('Validation Loss p_a: '     + str(np.mean(val_loss_p_a_array[epoch,:])))\n",
    "    print('Training   Loss vel: '     + str(np.mean(train_loss_vel_array[epoch,:])))\n",
    "    print('Validation Loss vel: '     + str(np.mean(val_loss_vel_array[epoch,:])))\n",
    "    print('Training   Accuracy p_a: ' + str(np.mean(train_metric_p_a_array[epoch,:])))\n",
    "    print('Validation Accuracy p_a: ' + str(np.mean(val_metric_p_a_array[epoch,:])))\n",
    "    print('Training   Accuracy vel: ' + str(np.mean(train_metric_vel_array[epoch,:])))\n",
    "    print('Validation Accuracy vel: ' + str(np.mean(val_metric_vel_array[epoch,:])))\n",
    "    print('Training Confusion Matrix Play:\\n')\n",
    "    print(train_p_conf_array[epoch, : , :])\n",
    "    print('')\n",
    "    print('Training Confusion Matrix Articulate:\\n')\n",
    "    print(train_a_conf_array[epoch, : , :])\n",
    "    print('')\n",
    "    print('Validation Confusion Matrix Play:\\n')\n",
    "    print(val_p_conf_array[epoch, : , :])\n",
    "    print('')\n",
    "    print('Validation Confusion Matrix Articulate:\\n')\n",
    "    print(val_a_conf_array[epoch, : , :])\n",
    "    print('')\n",
    "\n",
    "\n",
    "    time_new = time.time()\n",
    "    duration = time_new - time_old\n",
    "    time_old = time_new\n",
    "    print('Time: ' +  str(round(duration, 3)) + 's')\n",
    "\n",
    "\n",
    "    if (epoch + 1) in epoch_save_list:\n",
    "\n",
    "        # save model weights\n",
    "        iteration_name = 'epoch_' + str(epoch + 1) + '_dense_dropout'\n",
    "        save_path = Checkpoint_Directory + current_time_str[:-7] + '/' +  iteration_name + 'model'\n",
    "        lstm.save(save_path)\n",
    "\n",
    "        # save audios for train from y_pred_train, y_train\n",
    "        y_pred_note_train, y_pred_velocity_train, _ = lstm(X_train, training=False)\n",
    "        y_pred_train = tf.concat([y_pred_note_train, y_pred_velocity_train], axis=-1)\n",
    "        midi.generate_audio(y_pred_train, \n",
    "                            Music_Out_Training_Directory + current_time_str[:-7] + '/', \n",
    "                            iteration_name + 'train_pred', \n",
    "                            sample=True)\n",
    "        midi.generate_audio(y_train, \n",
    "                            Music_Out_Training_Directory + current_time_str[:-7] + '/', \n",
    "                            iteration_name + 'train_true', \n",
    "                            sample=False)\n",
    "        \n",
    "        # save audios for val from y_pred\n",
    "        y_pred_note_val, y_pred_velocity_val, _ = lstm(X_val, training=False)\n",
    "        y_pred_val = tf.concat([y_pred_note_val, y_pred_velocity_val], axis=-1)\n",
    "        midi.generate_audio(y_pred_val,    \n",
    "                            Music_Out_Training_Directory + current_time_str[:-7] + '/', \n",
    "                            iteration_name + 'val_pred', \n",
    "                            sample=True)\n",
    "        midi.generate_audio(y_val,    \n",
    "                            Music_Out_Training_Directory + current_time_str[:-7] + '/', \n",
    "                            iteration_name + 'val_true', \n",
    "                            sample=False)\n",
    "\n",
    "        # save the arrays \n",
    "        save_path = Numpy_Directory + current_time_str[:-7] + '/' \n",
    "        try:\n",
    "            os.mkdir(save_path)    \n",
    "        except:\n",
    "            print('destination folder exists')\n",
    "        np.savez(save_path + iteration_name + 'array', \n",
    "                 train_loss_p_a_array, \n",
    "                 train_loss_vel_array,\n",
    "                 train_metric_p_a_array,\n",
    "                 train_metric_vel_array,\n",
    "                 val_loss_p_a_array, \n",
    "                 val_loss_vel_array,\n",
    "                 val_metric_p_a_array,\n",
    "                 val_metric_vel_array,\n",
    "                 train_p_conf_array,\n",
    "                 train_a_conf_array,\n",
    "                 val_p_conf_array,\n",
    "                 val_a_conf_array,\n",
    "                 X_train.numpy(),\n",
    "                 y_train.numpy(),\n",
    "                 y_pred_train.numpy(),\n",
    "                 X_val.numpy(),\n",
    "                 y_val.numpy(),\n",
    "                 y_pred_val.numpy()\n",
    "                )\n",
    "    print('\\n\\n')\n",
    "\n",
    "total_time = time.time() - start_time \n",
    "print('Total time: ' + str(datetime.timedelta(seconds=total_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-confidence",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = Checkpoint_Directory + '/20210418/epoch_4_dense_dropoutmodel'\n",
    "model = tf.keras.models.load_model(load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-channels",
   "metadata": {},
   "source": [
    "#### Compare precitions to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-berkeley",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_labels = False # True\n",
    "if print_labels:\n",
    "    y_pred_note, y_pred_velocity, _ = model.predict(X)\n",
    "    y_pred = tf.concat([y_pred_note, y_pred_velocity], axis=-1)\n",
    "    tmp = tfp.distributions.Bernoulli(logits=y_pred[batch_idx,:,time_idx,0:2]).sample()\n",
    "    tmp2 = tf.cast(np_arrays['y_pred_train'][batch_idx,:, time_idx,2:3], dtype=tf.int32)\n",
    "    tmp3 = tf.concat([tmp,tmp2], axis=-1)\n",
    "    print(tf.concat([tf.cast(y[batch_idx,:, time_idx,:], dtype=tf.int32), tmp3], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-schedule",
   "metadata": {},
   "source": [
    "### MIDI generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate additional audios for each batch test audios for each batch\n",
    "add_for_each_batch = False  # True\n",
    "if add_for_each_batch:\n",
    "    n_train_batches = 10\n",
    "    n_val_batches = 2\n",
    "\n",
    "    random.seed(1337)\n",
    "    for n in range(n_train_batches):\n",
    "        print('Generate training audio: %d/%d' % (n + 1, n_train_batches), end='\\r')\n",
    "        train_dataset = prep.createDataSet(training_pieces, batch_size, num_timesteps, batch_size)\n",
    "        for _, (X, y) in enumerate(train_dataset):\n",
    "            y_pred_note, y_pred_velocity, _ = model.predict(X)\n",
    "            y_pred = tf.concat([y_pred_note, y_pred_velocity], axis=-1)\n",
    "            midi.generate_audio(y_pred,    Music_Out_Directory, current_time_str, 'batch_' + str(n) + '_train_pred', sample=True, tickscale=50)\n",
    "            midi.generate_audio(y,    Music_Out_Directory, current_time_str, 'batch_' + str(n) + '_train_true', sample=False, tickscale=50)\n",
    "\n",
    "    for n in range(n_val_batches):\n",
    "        print('Generate validation audio: %d/%d' % (n + 1, n_val_batches), end='\\r')\n",
    "        val_dataset   = prep.createDataSet(validation_pieces, batch_size, num_timesteps, batch_size)\n",
    "        for _, (X, y) in enumerate(val_dataset):\n",
    "            y_pred_note, y_pred_velocity, _ = model.predict(X)\n",
    "            y_pred = tf.concat([y_pred_note, y_pred_velocity], axis=-1)\n",
    "            midi.generate_audio(y_pred,    Music_Out_Directory, current_time_str, 'batch_' + str(n) + '_val_pred', sample=True, tickscale=50)\n",
    "            midi.generate_audio(y,    Music_Out_Directory, current_time_str, 'batch_' + str(n) + '_val_true' , sample=False, tickscale=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-gentleman",
   "metadata": {},
   "source": [
    "#### Genereate new MIDI files from scratch using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Music Generation from scratch\n",
    "\n",
    "num_notes = 88\n",
    "num_timesteps = 144\n",
    "n_beats = 48\n",
    "n_bars = 10\n",
    "t_gen = n_bars*n_beats\n",
    "batch_size_gen = 4\n",
    "num_timesteps_initial = 1 # start with initial Note_State_Batch with 't' dimension = 1 (can still a batch of samples run in parallel)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "notes_gen_initial = tf.zeros((batch_size_gen, num_notes, num_timesteps_initial, 3))\n",
    "\n",
    "# Initial States\n",
    "note_state_matrix_gen = notes_gen_initial\n",
    "\n",
    "\n",
    "# Generate note_state_matrix\n",
    "for t in tf.range(t_gen):\n",
    "    if(t<num_timesteps):\n",
    "        time_init = 0\n",
    "    else:\n",
    "        time_init = t%48\n",
    "    X  = prep.inputKernel(note_state_matrix_gen[:,:,-num_timesteps:,:], time_init=time_init)\n",
    "    _ , y_pred_velocity_train, y_pred_note_train = model.predict_on_batch(X)\n",
    "    new_note = tf.concat([y_pred_note_train[:,:,-1:,:], y_pred_velocity_train[:,:,-1:,:]], axis=-1)\n",
    "    new_note_p   = new_note[:,:,:,0]\n",
    "    new_note_a   = new_note[:,:,:,1] * new_note[:,:,:,0]\n",
    "    new_note_vel = new_note[:,:,:,2] * new_note[:,:,:,0]\n",
    "    new_note = tf.stack([new_note_p, new_note_a, new_note_vel], axis=-1)\n",
    "    note_state_matrix_gen = tf.concat([note_state_matrix_gen, new_note], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(batch_size_gen):\n",
    "    midi.generate_audio(note_state_matrix_gen[i:(i+1),:,:,:], \n",
    "                        Music_Out_Genereating_Directory + current_time_str[:-7] + '/',\n",
    "                        'generated' + '_batch_' + str(i) + '_epoch_4', \n",
    "                        sample=False,\n",
    "                        verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at feature vector\n",
    "prep.noteRNNInputSummary(prep.inputKernel(note_state_matrix_gen[:,:,0:144,:], time_init=0)[1,56,2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-seating",
   "metadata": {},
   "source": [
    "### Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'epoch_16_dense_dropoutarray.npz'\n",
    "npzfile = np.load(Numpy_Directory + '20210418' + '/' + filename) #  + current_time_str[:-7]\n",
    "keys = ['train_loss_p_a', 'train_loss_vel','train_metric_p_a', 'train_metric_vel', \n",
    "        'val_loss_p_a', 'val_loss_vel', 'val_metric_p_a', 'val_metric_vel', \n",
    "        'train_p_conf_array', 'train_a_conf_array','val_p_conf_array', 'val_a_conf_array',\n",
    "        'X_train', 'y_train', 'y_pred_train', 'X_val', 'y_val', 'y_pred_val'\n",
    "       ]\n",
    "np_arrays = {}\n",
    "for i in range(len(keys)):\n",
    "    np_arrays[keys[i]] = npzfile['arr_'+ str(i)] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-geography",
   "metadata": {},
   "source": [
    "#### Compare labels to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_idx = 30\n",
    "time_idx = 60\n",
    "batch_idx = 0\n",
    "\n",
    "print_labels = False # True\n",
    "if print_labels:\n",
    "    tmp = tfp.distributions.Bernoulli(logits=np_arrays['y_pred_train'][batch_idx,:,time_idx,0:2]).sample()\n",
    "    tmp2 = tf.cast(np_arrays['y_pred_train'][batch_idx,:, time_idx,2:3], dtype=tf.int32)\n",
    "    tmp3 = tf.concat([tmp,tmp2], axis=-1)\n",
    "    tf.concat([np_arrays['y_train'][batch_idx,:, time_idx,:], tmp3], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-minority",
   "metadata": {},
   "source": [
    "#### Visualisations - Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_trained = 16\n",
    "train_loss_p_a_avg = np.mean(np_arrays['train_loss_p_a'], axis=1)[0:max_trained]\n",
    "val_loss_p_a_avg = np.mean(np_arrays['val_loss_p_a'], axis=1)[0:max_trained]\n",
    "fig = px.line(y=[train_loss_p_a_avg,val_loss_p_a_avg])\n",
    "fig.data[0].name = \"train_loss_avg_p_a\"\n",
    "fig.data[1].name = \"val_loss_avg_p_a\"\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metric_avg = np.mean(np_arrays['train_metric_p_a'], axis=1)[0:max_trained]\n",
    "val_metric_avg = np.mean(np_arrays['val_metric_p_a'], axis=1)[0:max_trained]\n",
    "fig = px.line(y=[train_metric_avg, val_metric_avg])\n",
    "fig.data[0].name = \"train_metric_avg_p_a\"\n",
    "fig.data[1].name = \"val_metric_avg_p_a\"\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_vel_avg = np.mean(np_arrays['train_loss_vel'], axis=1)[0:max_trained]\n",
    "val_loss_vel_avg = np.mean(np_arrays['val_loss_vel'], axis=1)[0:max_trained]\n",
    "fig = px.line(y=[train_loss_vel_avg,val_loss_vel_avg])\n",
    "fig.data[0].name = \"train_loss_avg_vel\"\n",
    "fig.data[1].name = \"val_loss_avg_vel\"\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metric_avg = np.mean(np_arrays['train_metric_vel'], axis=1)[0:max_trained]\n",
    "val_metric_avg = np.mean(np_arrays['val_metric_vel'], axis=1)[0:max_trained]\n",
    "fig = px.line(y=[train_metric_avg, val_metric_avg])\n",
    "fig.data[0].name = \"train_metric_avg_vel\"\n",
    "fig.data[1].name = \"val_metric_avg_vel\"\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_conf_percent = np_arrays['train_p_conf_array'][0:max_trained] / np.sum(np_arrays['train_p_conf_array'][0:max_trained], axis=1, keepdims=True)\n",
    "\n",
    "fig = px.line(y=[p_conf_percent[:,0,0],p_conf_percent[:,1,0], p_conf_percent[:,0,1], p_conf_percent[:,1,1]])\n",
    "fig.data[0].name = \"pred_not_played_true_not_played\"\n",
    "fig.data[1].name = \"pred_played_true_not_played\"\n",
    "fig.data[2].name = \"pred_not_played_true_played\"\n",
    "fig.data[3].name = \"pred_played_true_played\"\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_conf_percent = np_arrays['val_p_conf_array'][0:max_trained]  / np.sum(np_arrays['val_p_conf_array'][0:max_trained] , axis=1, keepdims=True)\n",
    "\n",
    "fig = px.line(y=[p_conf_percent[:,0,0],p_conf_percent[:,1,0], p_conf_percent[:,0,1], p_conf_percent[:,1,1]])\n",
    "fig.data[0].name = \"pred_not_played_true_not_played\"\n",
    "fig.data[1].name = \"pred_played_true_not_played\"\n",
    "fig.data[2].name = \"pred_not_played_true_played\"\n",
    "fig.data[3].name = \"pred_played_true_played\"\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_conf_percent = np_arrays['train_a_conf_array'][0:max_trained]  / np.sum(np_arrays['train_a_conf_array'][0:max_trained] , axis=1, keepdims=True)\n",
    "\n",
    "fig = px.line(y=[a_conf_percent[:,0,0],a_conf_percent[:,1,0], a_conf_percent[:,0,1], a_conf_percent[:,1,1]])\n",
    "fig.data[0].name = \"pred_not_articulated_true_not_articulated\"\n",
    "fig.data[1].name = \"pred_articulated_true_not_articulated\"\n",
    "fig.data[2].name = \"pred_not_articulated_true_articulated\"\n",
    "fig.data[3].name = \"pred_articulated_true_articulated\"\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_conf_percent = np_arrays['val_a_conf_array'][0:max_trained]  / np.sum(np_arrays['val_a_conf_array'][0:max_trained] , axis=1, keepdims=True)\n",
    "\n",
    "fig = px.line(y=[a_conf_percent[:,0,0],a_conf_percent[:,1,0], a_conf_percent[:,0,1], a_conf_percent[:,1,1]])\n",
    "fig.data[0].name = \"pred_not_articulated_true_not_articulated\"\n",
    "fig.data[1].name = \"pred_articulated_true_not_articulated\"\n",
    "fig.data[2].name = \"pred_not_articulated_true_articulated\"\n",
    "fig.data[3].name = \"pred_articulated_true_articulated\"\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
